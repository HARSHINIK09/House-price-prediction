{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra{\"cells\":[{\"metadata\":{\"trusted\":true},\"cell_type\":\"code\",\"source\":\"# This Python 3 environment comes with many helpful analytics libraries installed\\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\\n# For example, here's several helpful packages to load in \\n\\nimport numpy as np # linear algebra\\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\\n\\n# Input data files are available in the \\\"../input/\\\" directory.\\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\\n\\nimport os\\nfor dirname, _, filenames in os.walk('/kaggle/input'):\\n    for filename in filenames:\\n        print(os.path.join(dirname, filename))\\n\\n# Any results you write to the current directory are saved as output.\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"trusted\":true},\"cell_type\":\"code\",\"source\":\"train = pd.read_csv('../input/house-prices-advanced-regression-techniques/train.csv')\\ntest = pd.read_csv('../input/house-prices-advanced-regression-techniques/test.csv')\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"trusted\":true},\"cell_type\":\"code\",\"source\":\"train1 = train #make a copy\\ntest1 = test\\ntrain1 = train1.drop(\\\"Id\\\", axis = 1)\\ntest1 = test1.drop(\\\"Id\\\", axis = 1)\\nneighbour_feat = train['Neighborhood']\\nsalePrice_arr = train['SalePrice']\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"trusted\":true},\"cell_type\":\"code\",\"source\":\"fill_none = [ \\\"MiscFeature\\\",\\\"MasVnrType\\\"] #fill none from the given data\\nfor col in fill_none:\\n    train1[col] = train1[col].fillna(\\\"None\\\")\\n    test1[col] = test1[col].fillna(\\\"None\\\")\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"trusted\":true},\"cell_type\":\"code\",\"source\":\"fill_na = [\\\"GarageType\\\", \\\"GarageFinish\\\", \\\"GarageQual\\\",\\\"FireplaceQu\\\",\\\"PoolQC\\\",\\\"Alley\\\", \\n           \\\"BsmtQual\\\", \\\"BsmtCond\\\", \\\"BsmtExposure\\\", \\\"BsmtFinType1\\\",\\\"BsmtFinType2\\\",\\n                    \\\"GarageCond\\\", \\\"Fence\\\"]\\nfor col in fill_na:\\n    train1[col] = train1[col].fillna(\\\"NA\\\")\\n    test1[col] = test1[col].fillna(\\\"NA\\\")\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"trusted\":true},\"cell_type\":\"code\",\"source\":\"dict_moSold = {1 : \\\"Jan\\\", 2 : \\\"Feb\\\", 3 : \\\"Mar\\\", 4 : \\\"Apr\\\", 5 : \\\"May\\\", 6 : \\\"Jun\\\", 7 : \\\"Jul\\\", 8 : \\\"Aug\\\", 9 : \\\"Sep\\\", 10 : \\\"Oct\\\",\\n         11 : \\\"Nov\\\", 12 : \\\"Dec\\\"}\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"trusted\":true},\"cell_type\":\"code\",\"source\":\"col_missing = []\\ncol_missing_test = []\\n\\nfor col in train1.columns:\\n    if len(np.unique(train1.isnull()))>1:\\n        col_missing.append(col)\\nfor col_miss in col_missing:\\n    if train1[col_miss].dtype=='int64':\\n        train1[col_miss] = np.nanmean(train1[col_miss])\\n    if train1[col_miss].dtype=='float64':\\n        train1[col_miss] = np.nanmean(train1[col_miss])\\n\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"trusted\":true},\"cell_type\":\"code\",\"source\":\"        \\nfor col in test1.columns:\\n    if len(np.unique(test1.isnull()))>1:\\n        col_missing_test.append(col)\\nfor col_miss in col_missing_test:\\n    if test1[col_miss].dtype=='int64':\\n        test1[col_miss] = np.nanmean(test1[col_miss])\\n    if test1[col_miss].dtype=='float64':\\n        test1[col_miss] = np.nanmean(test1[col_miss])\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"trusted\":true},\"cell_type\":\"code\",\"source\":\"\\ndict_msClass = {20 : \\\"Cat_20\\\", 30 : \\\"Cat_30\\\", 40 : \\\"Cat_40\\\", 45 : \\\"Cat_45\\\",50 : \\\"Cat_50\\\", 60 : \\\"Cat_60\\\", 70 : \\\"Cat_70\\\", 75 : \\\"Cat_75\\\",\\n         80 : \\\"Cat_80\\\", 85 : \\\"Cat_85\\\", 90 : \\\"Cat_90\\\", 120 : \\\"Cat_120\\\", 150 : \\\"Cat_150\\\", 160 : \\\"Cat_160\\\", 180 : \\\"Cat_180\\\",\\n         190 : \\\"Cat_190\\\"}\\n\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"trusted\":true},\"cell_type\":\"code\",\"source\":\"train1 = train1.replace({\\\"MSSubClass\\\" : dict_msClass,\\\"MoSold\\\" : dict_moSold})\\ntest1 = test1.replace({\\\"MSSubClass\\\" : dict_msClass,\\\"MoSold\\\" : dict_moSold})\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"trusted\":true},\"cell_type\":\"code\",\"source\":\"train1.drop(\\\"LotFrontage\\\", axis=1, inplace=True)\\ntest1.drop(\\\"LotFrontage\\\", axis=1, inplace=True)\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"trusted\":true},\"cell_type\":\"code\",\"source\":\"mode_fill = ['Functional','KitchenQual','MSZoning','SaleType','Utilities','Electrical', 'Exterior1st', 'Exterior2nd']\\nfor col in mode_fill:\\n    train1[col]=train1[col].fillna(train1[col].mode()[0])\\n    test1[col]=test1[col].fillna(test1[col].mode()[0])\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"trusted\":true},\"cell_type\":\"code\",\"source\":\"train1[\\\"FinalBath\\\"] = train1[\\\"FullBath\\\"] + ((1/2) * train1[\\\"HalfBath\\\"]) + train1[\\\"BsmtFullBath\\\"] + ((1/2)*train1[\\\"BsmtHalfBath\\\"])\\ntest1[\\\"FinalBath\\\"] = test1[\\\"FullBath\\\"] + ((1/2) * test1[\\\"HalfBath\\\"]) + test1[\\\"BsmtFullBath\\\"] + ((1/2)*test1[\\\"BsmtHalfBath\\\"])\\n\\ntrain1[\\\"FinalSF\\\"] = train1[\\\"GrLivArea\\\"] + train1[\\\"TotalBsmtSF\\\"]\\ntest1[\\\"FinalSF\\\"] = test1[\\\"GrLivArea\\\"] + test1[\\\"TotalBsmtSF\\\"]\\n\\ntrain1[\\\"FinalFlrSF\\\"] = train1[\\\"1stFlrSF\\\"] + train1[\\\"2ndFlrSF\\\"]\\ntest1[\\\"FinalFlrSF\\\"] = test1[\\\"1stFlrSF\\\"] + test1[\\\"2ndFlrSF\\\"]\\n\\ntrain1[\\\"FinalPorchSF\\\"] = train1[\\\"3SsnPorch\\\"] + train1[\\\"ScreenPorch\\\"] + train1[\\\"OpenPorchSF\\\"] + train1[\\\"EnclosedPorch\\\"]\\ntest1[\\\"FinalPorchSF\\\"] = test1[\\\"3SsnPorch\\\"] + test1[\\\"ScreenPorch\\\"] + test1[\\\"OpenPorchSF\\\"] + test1[\\\"EnclosedPorch\\\"]\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"trusted\":true},\"cell_type\":\"code\",\"source\":\"train_label = np.log(train1['SalePrice'])\\ntrain1.drop(['SalePrice'], axis = 1, inplace=True)\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"trusted\":true},\"cell_type\":\"code\",\"source\":\"categ_train = train1.select_dtypes(include=[np.object])\\ncateg_test = train1.select_dtypes(include=[np.object])\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"trusted\":true},\"cell_type\":\"code\",\"source\":\"categ_train_test = pd.concat([categ_train,categ_test])\\ncateg_onehot = pd.get_dummies(categ_train_test, columns=categ_train_test.columns)\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"trusted\":true},\"cell_type\":\"code\",\"source\":\"categ_train_encod = categ_onehot[:categ_train.shape[0]]\\ncateg_test_encod = categ_onehot[categ_train.shape[0]:]\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"trusted\":true},\"cell_type\":\"code\",\"source\":\"train_final = pd.concat([train1, categ_train_encod],axis=1)\\ntest_final = pd.concat([test1, categ_test_encod],axis=1)\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"trusted\":true},\"cell_type\":\"code\",\"source\":\"train_final = train_final.select_dtypes(exclude=['object'])\\ntest_final = test_final.select_dtypes(exclude=['object'])\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{},\"cell_type\":\"markdown\",\"source\":\"# Homework 3 - Ames Housing Dataset\",\"execution_count\":null},{\"metadata\":{},\"cell_type\":\"markdown\",\"source\":\"For all parts below, answer all parts as shown in the Google document for Homework 3. Be sure to include both code that justifies your answer as well as text to answer the questions. We also ask that code be commented to make it easier to follow.\",\"execution_count\":null},{\"metadata\":{},\"cell_type\":\"markdown\",\"source\":\"## Part 1 - Pairwise Correlations\",\"execution_count\":null},{\"metadata\":{\"trusted\":true},\"cell_type\":\"code\",\"source\":\"# TODO: show visualization\\n#correlation matrix\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\ncorrmat = train.corr(method='pearson')\\nf, ax = plt.subplots(figsize=(14, 11))\\nsns.heatmap(corrmat,vmin=-0.5, vmax=0.8, square=True);\\n\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{},\"cell_type\":\"markdown\",\"source\":\"**Pearson Correlation for 16 most interesting variables. The top 8 variables which are having best positive correlation and top 8 variables that have the best negative correlation are chosen and heatmap is plotted below**\",\"execution_count\":null},{\"metadata\":{\"trusted\":true},\"cell_type\":\"code\",\"source\":\"k = 8 #number of variables for heatmap\\ncols = corrmat.nlargest(k, 'SalePrice')['SalePrice'].index\\nsm_value = corrmat.nsmallest(k, 'SalePrice')['SalePrice'].index\\nsm_la_col = list(cols)+list(sm_value)\\ncm = np.corrcoef(train[sm_la_col].values.T)\\nsns.set(font_scale=1.05)\\nf, ax = plt.subplots(figsize=(14, 11))\\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 11}, yticklabels=sm_la_col, xticklabels=sm_la_col, cmap=\\\"YlGnBu\\\")\\nplt.show()\\n\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{},\"cell_type\":\"markdown\",\"source\":\"Discuss most positive and negative correlations.\",\"execution_count\":null},{\"metadata\":{\"trusted\":true},\"cell_type\":\"code\",\"source\":\"sm_la_col #list of 16 of the most interesting variables\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{},\"cell_type\":\"markdown\",\"source\":\"**Out of 16 features, the following were found to be extremes when pearson correlation is applied\\nHighest correlation: 0.88 between GarageArea and GarageCars\\nLowest correlation: -0.25 between MSSubClass and 1stFlrSF**\",\"execution_count\":null},{\"metadata\":{},\"cell_type\":\"markdown\",\"source\":\"## Part 2 - Informative Plots\",\"execution_count\":null},{\"metadata\":{},\"cell_type\":\"markdown\",\"source\":\"What interesting properties does Plot 1 reveal?\",\"execution_count\":null},{\"metadata\":{\"trusted\":true},\"cell_type\":\"code\",\"source\":\"import seaborn as sns\\ntrain = pd.read_csv('../input/house-prices-advanced-regression-techniques/train.csv')\\ntest = pd.read_csv('../input/house-prices-advanced-regression-techniques/test.csv')\\nsns.set_palette(\\\"husl\\\")\\nsns.set(style=\\\"darkgrid\\\")\\nplt.scatter(train['GrLivArea'], train['SalePrice'])\\nplt.title(\\\"Above Ground Living Area Vs SalePrice\\\")\\nplt.xlabel(\\\"Above Ground Living Area\\\")\\nplt.ylabel(\\\"Sale Price\\\")\\nplt.show()\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{},\"cell_type\":\"markdown\",\"source\":\"**From the above plot, It is evident that, For values of the GrLivArea that are greater than 4000, linear relation is deviated for the 2 points.Thus they are the outliers.\\nThe outliers imply that the 2 points might correspond to unused property or the property that is not related to general housing.\\n**\",\"execution_count\":null},{\"metadata\":{\"trusted\":true},\"cell_type\":\"code\",\"source\":\"sns.set_palette(\\\"husl\\\")\\nquality = train['SalePrice'].groupby(train['OverallQual']).mean()\\nplt.bar(range(0,10),quality)\\nplt.xlabel('Quality of house')\\nplt.ylabel(' mean Sale Price')\\nplt.title('Quality of house vs mean Sale Price')\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{},\"cell_type\":\"markdown\",\"source\":\"**The SalePrice increased with Overall Quality as expected. SalePrice follows a steep increasing trend with Overall Quality**\",\"execution_count\":null},{\"metadata\":{\"trusted\":true},\"cell_type\":\"code\",\"source\":\"sns.set_palette(\\\"cubehelix\\\")\\nsns.catplot(x=\\\"OverallCond\\\", y=\\\"SalePrice\\\",\\n            kind=\\\"box\\\", dodge=False, data=train, height=10, palette=sns.color_palette(\\\"coolwarm\\\", 8))\\nplt.xticks(rotation=45);\\nplt.title('OverallCond Vs SalePrice')\\n\\nplt.figure(figsize = (10, 6))\\nsns.countplot(x ='OverallCond', data = train, palette=sns.color_palette(\\\"coolwarm\\\", 8))\\nplt.xticks(rotation=45)\\nplt.title('OverallCond')\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{},\"cell_type\":\"markdown\",\"source\":\"**Overall Condition at 5 seems to be in majority, probably the reason for which there's high SalePrice. The SalePrice seems to increase with Overall Condition on a whole**\",\"execution_count\":null},{\"metadata\":{\"trusted\":true},\"cell_type\":\"code\",\"source\":\"neighborhood = np.unique(train[\\\"Neighborhood\\\"])\\navg_list_neighbors=[]\\nfor neigh in neighborhood:\\n    avg_list_neighbors.append(np.mean(train[train[\\\"Neighborhood\\\"]\\n                                       ==neigh][\\\"SalePrice\\\"]))\\nplt.plot(neighborhood, avg_list_neighbors)\\nplt.xticks(rotation=90);\\nplt.xlabel('Neighborhood')\\nplt.ylabel('Average')\\nplt.title('NeighborHood Vs Average value of Saleprice')\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{},\"cell_type\":\"markdown\",\"source\":\"**From the above plot we can observe that NoRidge,DridgHt, StoneBr have maximum average value of saleprice compared to all other neighborhoods. **\",\"execution_count\":null},{\"metadata\":{\"trusted\":true},\"cell_type\":\"code\",\"source\":\"\\ntrain.plot(x='MSSubClass', y='SalePrice', kind = 'scatter',xticks=range(0,190,10),figsize = (10, 6))\\n\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{},\"cell_type\":\"markdown\",\"source\":\"** From the given data,\\n 20 indicates One storied 1946 and newer all styles\\n 60 indicates two storied 1946 and newer\\n 120 indicate one storied Planned Unit Development - 1946 and newer.\\n MS_SubClass-60 is having higher saleprice compared to other sub-classes followed by MS_SubClass-20 and  MS_SubClass-120.**\",\"execution_count\":null},{\"metadata\":{},\"cell_type\":\"markdown\",\"source\":\"## Part 3 - Handcrafted Scoring Function\",\"execution_count\":null},{\"metadata\":{\"trusted\":true},\"cell_type\":\"code\",\"source\":\"import numpy as np\\nscore_params = ['GrLivArea', 'YearBuilt', 'GarageArea', 'OverallCond']\\ndef score(data,params):\\n    return ((data[params[0]]/(np.mean(data[params[0]])))**2 + data[params[1]]/(np.mean(data[params[1]])) + \\\\\\n            data[params[2]]/(np.mean(data[params[2]])) + (data[params[3]]/(np.mean(data[params[3]]))))\\ntrain['desirability'] = score(train,score_params)\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"trusted\":true},\"cell_type\":\"code\",\"source\":\"trainsortdec = train.sort_values(by=['desirability'], ascending=False)\\ntrainsortdec.head(10)\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{},\"cell_type\":\"markdown\",\"source\":\"What is the ten most desirable houses?\\n* ** \\\"high ground(grade) living area, the year built, garage area, overall condition\\\" are considered highly desirable attributes. the above 10 are the most desirable based on these attributes.Their saleprice is in good terms with desirability**\",\"execution_count\":null},{\"metadata\":{\"trusted\":true},\"cell_type\":\"code\",\"source\":\"trainsort = train.sort_values(by=['desirability'])\\ntrainsort.head(10)\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{},\"cell_type\":\"markdown\",\"source\":\"What is the ten least desirable houses?\\n* ** \\\"low ground(grade) living area, the year built, garage area, overall condition\\\" are considered least desirable attributes.the above table are the ten least desirable based on these factors.Their saleprice is in good terms with desirability and Not very well for low desirable data.**\",\"execution_count\":null},{\"metadata\":{\"trusted\":true},\"cell_type\":\"code\",\"source\":\"train['SalePrice'].corr(train['desirability'])\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{},\"cell_type\":\"markdown\",\"source\":\"Describe your scoring function and how well you think it worked.\\n* **\\n* **As per the above correlation between the desirability with sale price is good, although it needn't be true always. We defined a house desirable having large spaces. With 0.737(rounded) correlation, the scoring function appears to have done fine.****\",\"execution_count\":null},{\"metadata\":{},\"cell_type\":\"markdown\",\"source\":\"## Part 4 - Pairwise Distance Function\",\"execution_count\":null},{\"metadata\":{\"trusted\":true},\"cell_type\":\"code\",\"source\":\"len(train_final.values[0])\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"trusted\":true},\"cell_type\":\"code\",\"source\":\"import pandas as pd\\nfrom sklearn import preprocessing\\nfrom sklearn.decomposition import PCA\\nfrom sklearn.manifold import TSNE\\n\\ndef pairwise_euclidean(a,b):\\n    if len(a) == len(b):\\n        return np.sqrt(sum([(x-y)**2 for x,y in zip(a,b)]))\\n    else:\\n        return False\\n\\nx = train_final.values\\nmin_max_scaler = preprocessing.MinMaxScaler()\\nx_scaled = min_max_scaler.fit_transform(x)\\ntrain = pd.DataFrame(x_scaled,columns=train_final.columns)\\npca = PCA(n_components=50)\\npcaComponents = pca.fit_transform(train)\\npcaDf = pd.DataFrame(data = pcaComponents)\\nX_embed = TSNE(n_components=2).fit_transform(pcaDf)\\n\\npairwiseds = []\\nfor i in range(len(X_embed)):\\n    ds = []\\n    for j in range(len(X_embed)):\\n        ds.append(pairwise_euclidean(X_embed[i], X_embed[j]))\\n    pairwiseds.append(ds)\\n\\ntrue_count = 0\\nfalse_count = 0\\nfor i in range(len(pairwiseds)):\\n    original_arr = pairwiseds[i]\\n    sorted_arr = np.sort(pairwiseds[i])\\n    found_elem = np.where(original_arr == sorted_arr[1])[0]\\n    if neighbour_feat[found_elem[0]]==neighbour_feat[i]:\\n        true_count=true_count+1\\n    else:\\n        false_count=false_count+1\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"trusted\":true},\"cell_type\":\"code\",\"source\":\"(true_count/(true_count+false_count))*100\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{},\"cell_type\":\"markdown\",\"source\":\"How well does the distance function work? When does it do well/badly?\\n* \\n**The function got approximately 52% match with neighbourhood which tells that it has done well.\\nMin-Max Scalar Normalization technique is used. Dimensionality reduction is done using PCA and reduced 300+ dimensions to 50.Used t-sne to reduce dimensions from 50 to 2.Obtained pair wise distance vector using handcrafted function.Validate whether its nearest distance point contains same neighborhood. If neighborhood is same, increment true_count to true.Approximately 52% of the neighbourhood matches the nearest. **\",\"execution_count\":null},{\"metadata\":{},\"cell_type\":\"markdown\",\"source\":\"## Part 5 - Clustering\",\"execution_count\":null},{\"metadata\":{\"trusted\":true},\"cell_type\":\"code\",\"source\":\"df = pd.DataFrame(X_embed, columns=['xcoord','ycoord'])\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"trusted\":true},\"cell_type\":\"code\",\"source\":\"#plt.scatter(df['xcoord'], df['ycoord'],c='DarkBlue')\\nplt.figure(figsize=[12,8])\\nsns.scatterplot(x=\\\"xcoord\\\", y=\\\"ycoord\\\",data=df)\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"trusted\":true},\"cell_type\":\"code\",\"source\":\"from sklearn.cluster import KMeans\\nkmeans = KMeans(n_clusters=9, random_state=0).fit(X_embed)\\nlabels = kmeans.labels_\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"trusted\":true},\"cell_type\":\"code\",\"source\":\"df['labels'] = labels\\nplt.figure(figsize=[12,8])\\nsns.scatterplot(x=\\\"xcoord\\\", y=\\\"ycoord\\\",hue=\\\"labels\\\",legend=\\\"full\\\",palette=sns.color_palette(\\\"hls\\\", 9),data=df)\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"trusted\":true},\"cell_type\":\"code\",\"source\":\"import numpy as np\\nimport operator\\n#df_main = pd.DataFrame()\\nfor i in range(0,9):\\n    neighbor = neighbour_feat[labels==i]\\n    unique, counts = np.unique(neighbor, return_counts=True)\\n    d = dict(zip(unique, counts))\\n    sorted_d = sorted(d.items(), key=operator.itemgetter(1),reverse=True)\\n    df_ = pd.DataFrame({'clustercentre': i,'Neighborhood': sorted_d[0][0],'count':sorted_d[0][1],\\n                       'percent':(sorted_d[0][1]/sum(d.values()))*100}, index = [i])\\n    print(df_)\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"trusted\":true},\"cell_type\":\"code\",\"source\":\"from sklearn.cluster import AgglomerativeClustering\\nclustering = AgglomerativeClustering(affinity='precomputed', compute_full_tree='auto',\\n                       connectivity=None, distance_threshold=None,\\n                       linkage='single', memory=None, n_clusters=9,\\n                       pooling_func='deprecated').fit(pairwiseds)\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"trusted\":true},\"cell_type\":\"code\",\"source\":\"df.head()\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"trusted\":true},\"cell_type\":\"code\",\"source\":\"df['pre_labels'] = clustering.labels_\\npre_labels = clustering.labels_\\nplt.figure(figsize=[12,8])\\nsns.scatterplot(x=\\\"xcoord\\\", y=\\\"ycoord\\\",hue=\\\"pre_labels\\\",legend=\\\"full\\\",palette=sns.color_palette(\\\"hls\\\", 9),data=df)\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"trusted\":true},\"cell_type\":\"code\",\"source\":\"import numpy as np\\nimport operator\\n#df_main = pd.DataFrame()\\nfor i in range(0,9):\\n    neighbor = neighbour_feat[pre_labels==i]\\n    unique, counts = np.unique(neighbor, return_counts=True)\\n    d = dict(zip(unique, counts))\\n    sorted_d = sorted(d.items(), key=operator.itemgetter(1),reverse=True)\\n    df_ = pd.DataFrame({'clustercentre': i,'Neighborhood': sorted_d[0][0],'count':sorted_d[0][1],\\n                       'percent':(sorted_d[0][1]/sum(d.values()))*100}, index = [i])\\n    print(df_)\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{},\"cell_type\":\"markdown\",\"source\":\"How well do the clusters reflect neighborhood boundaries? Write a discussion on what your clusters capture and how well they work.\\n\\n**We applied two algorithms here, kmeans and Agglomerative. Kmeans takes input as X_embed(reduced dimensionality), which is the output of tsne. Kmeans makes sure every cluster is uniformly distributed. However, we have repeating maximum neighborhoods. In this case for example, we have 'NAmes', coming in 3 clusters'. Agglomerative takes distance matrix as input and cluster labels, from which we determine the cluster it belongs too. As we can see from graphs above, agglomerative does non-uniform clustering with 1 cluster taking all 'NAmes' data. However, the clusters have a clear separation of the neighborhoods .**\",\"execution_count\":null},{\"metadata\":{},\"cell_type\":\"markdown\",\"source\":\"## Part 6 - Linear Regression\",\"execution_count\":null},{\"metadata\":{\"trusted\":true},\"cell_type\":\"code\",\"source\":\"from sklearn import linear_model\\nfrom sklearn import metrics\\nfrom sklearn.preprocessing import LabelEncoder\\n\\ntrain2 = pd.read_csv('../input/house-prices-advanced-regression-techniques/train.csv')\\nlinear_model = linear_model.LinearRegression()\\n#x = x.drop(['Id','SalePrice'],axis=1)\\nx = train2\\nx = x.drop(['Id','SalePrice'],axis=1)\\ny = salePrice_arr\\n\\ncol_miss = [col for col in x.columns if x[col].isnull().any()]\\nfor col in col_miss:\\n    if(x[col].dtype == np.dtype('object')):\\n         x[col]=x[col].fillna(x[col].value_counts().index[0])\\n    else:\\n        x[col] = x[col].fillna(x[col].mean()) \\nle = LabelEncoder()\\nfor col in x.select_dtypes(include=['object']):\\n    x[col] = le.fit_transform(x[col])\\n    \\nmodel = linear_model.fit(x,y)\\ny_fit = model.predict(x)\\nRMSE = np.sqrt(metrics.mean_squared_error(y, y_fit))\\nRMSE\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"trusted\":true},\"cell_type\":\"code\",\"source\":\"y_ = np.log(y)\\nmodel = linear_model.fit(x,y_)\\ny_fit = model.predict(x)\\nRMSE = np.sqrt(metrics.mean_squared_error(y_, y_fit))\\nRMSE\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"trusted\":true},\"cell_type\":\"code\",\"source\":\"import operator\\ncoeffs = model.coef_\\nmax_index, max_value = max(enumerate(coeffs), key=operator.itemgetter(1))\\nmin_index, min_value = min(enumerate(coeffs), key=operator.itemgetter(1))\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"trusted\":true},\"cell_type\":\"code\",\"source\":\"x.columns[max_index]\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"trusted\":true},\"cell_type\":\"code\",\"source\":\"max_value\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"trusted\":true},\"cell_type\":\"code\",\"source\":\"x.columns[min_index]\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"trusted\":true},\"cell_type\":\"code\",\"source\":\"min_value\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{},\"cell_type\":\"markdown\",\"source\":\"How well/badly does it work? Which are the most important variables?\\n* \\n**The initial rmse value is 30000 which isn't a good value. To yeild better results the fitting against the log of the value and label encoding is applied thus rmse got reduced. The new rmse value is approximately equal to 0.1335. poolqc has the highest absolute value of coefficient and Street has the highest positive value.High weights contribute a lot to the model and hence are most important**\",\"execution_count\":null},{\"metadata\":{},\"cell_type\":\"markdown\",\"source\":\"## Part 7 - External Dataset\",\"execution_count\":null},{\"metadata\":{\"trusted\":true},\"cell_type\":\"code\",\"source\":\"import pandas as pd\\ntrain3 = pd.read_csv('../input/house-prices-advanced-regression-techniques/train.csv')\\n\\nx = train3\\nnational_burglary_rate = 430.4\\nnational_rape_rate = 41.7\\nnational_murder_rate = 5.3\\n#In case of missing data, we substituted with Ames city average\\n#https://www.addressreport.com/report/neighborhood/ames-ia/bloomington-heights-ames-ia/\\n\\n#first five are household_income,owners,property_tax and remaining are understood\\n\\ndict_ = {}\\n\\names_avg = [46358,0.44,2479,\\n                (1-0.30)*national_burglary_rate,(1-0.0)*national_rape_rate,\\n                (1-0.81)*national_murder_rate]\\ndict_['Blmngtn'] = [95256,0.84,3742,\\n                      (1-0.05)*national_burglary_rate,(1-0.54)*national_rape_rate,\\n                      (1-0.79)*national_murder_rate]\\ndict_['Blueste'] = ames_avg\\ndict_['BrDale'] = [45558,0.40,1785,\\n                      (1-0.14)*national_burglary_rate,(1-0.26)*national_rape_rate,\\n                      (1-0.89)*national_murder_rate]\\ndict_['BrkSide'] = ames_avg\\ndict_['ClearCr'] = ames_avg\\ndict_['CollgCr'] = [66875,0.55,2616,\\n                    (1-0.26)*national_burglary_rate,(1+0.11)*national_rape_rate,\\n                (1-0.92)*national_murder_rate]\\ndict_['Crawfor'] = ames_avg\\ndict_['Edwards'] = ames_avg\\ndict_['Gilbert'] = ames_avg\\ndict_['IDOTRR'] = ames_avg\\ndict_['MeadowV'] = [53962,0.51,1521,\\n            (1-0.52)*national_burglary_rate,(1+0.22)*national_rape_rate,\\n            (1-0.78)*national_murder_rate]\\ndict_['Mitchel'] = ames_avg\\ndict_['NAmes'] = ames_avg\\ndict_['NoRidge'] = ames_avg\\ndict_['NPkVill'] = ames_avg\\ndict_['NridgHt'] = [95256,0.84,5478,\\n            (1-0.05)*national_burglary_rate,(1-0.54)*national_rape_rate,\\n            (1-0.79)*national_murder_rate]\\ndict_['NWAmes'] = ames_avg\\ndict_['OldTown'] = ames_avg\\ndict_['SWISU'] = ames_avg\\ndict_['Sawyer'] = ames_avg\\ndict_['SawyerW'] = ames_avg\\ndict_['Somerst'] = [84600,0.62,2314,\\n            (1+0.09)*national_burglary_rate,(1-0.39)*national_rape_rate,\\n            (1-0.78)*national_murder_rate]\\ndict_['StoneBr'] = [95256,0.84,4369,\\n            (1-0.05)*national_burglary_rate,(1-0.54)*national_rape_rate,\\n            (1-0.79)*national_murder_rate]\\ndict_['Timber'] = ames_avg\\ndict_['Veenker'] = ames_avg\\nx['household_income'] = 0\\nx['owners'] = 0\\nx['property_tax'] = 0\\nx['burglary_rate'] = 0\\nx['rape_rate'] = 0\\nx['murder_rate'] = 0\\nkeyset = ['household_income','owners','property_tax','burglary_rate','rape_rate','murder_rate']\\nhousehold_arr = []\\nowners_arr = []\\nproperty_tax_arr=[]\\nburglary_rate_arr=[]\\nrape_rate_arr=[]\\nmurder_rate_arr=[]\\nfor i in range(len(x)):\\n    neighborhood = x.iloc[i]['Neighborhood']\\n    household_arr.append(dict_[neighborhood][0])\\n    owners_arr.append(dict_[neighborhood][1])\\n    property_tax_arr.append(dict_[neighborhood][2])\\n    burglary_rate_arr.append(dict_[neighborhood][3])\\n    rape_rate_arr.append(dict_[neighborhood][4])\\n    murder_rate_arr.append(dict_[neighborhood][5])\\nx['household_income']=household_arr\\nx['owners']=owners_arr\\nx['property_tax']=property_tax_arr\\nx['burglary_rate']=burglary_rate_arr\\nx['rape_rate']=rape_rate_arr\\nx['murder_rate']=murder_rate_arr\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"trusted\":true},\"cell_type\":\"code\",\"source\":\"#Baseline Model\\nfrom sklearn.preprocessing import LabelEncoder\\nfrom sklearn import metrics\\nfrom scipy import stats \\nfrom sklearn.linear_model import Ridge\\nfrom sklearn.model_selection import cross_val_score\\nimport base64\\nimport numpy as np\\nfrom sklearn.preprocessing import LabelEncoder\\nfrom sklearn import linear_model\\nfrom sklearn import metrics\\n\\ncol_miss = [col for col in x.columns if x[col].isnull().any()]\\nfor col in col_miss:\\n    if(x[col].dtype == np.dtype('object')):\\n         x[col]=x[col].fillna(x[col].value_counts().index[0])\\n    else:\\n        x[col] = x[col].fillna(x[col].mean()) \\nle = LabelEncoder()\\nfor col in x.select_dtypes(include=['object']):\\n    x[col] = le.fit_transform(x[col])\\n    \\ny = x[\\\"SalePrice\\\"]\\ny = np.log(y+1)\\nx = x.drop(['Id','SalePrice'],axis=1)\\nlm = linear_model.LinearRegression()\\nmodel = lm.fit(x,y)\\ny_pred = model.predict(x)\\nRMSE = np.sqrt(metrics.mean_squared_error(y, y_pred))\\nprint(RMSE)\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{},\"cell_type\":\"markdown\",\"source\":\"Describe the dataset and whether this data helps with prediction.\\n* \\n** The above dataset is procured from #https://en.wikipedia.org/wiki/Crime_in_the_United_States.  the data  is processed using https://www.addressreport.com and extracted values for attributes like household_income,owners,property_tax, burglary_rate,rape_rate, murder_rate for available neighborhoods of the Ames. The data that is missing is replaced with the city average. The Linear regression model is then applied after appending the attributes obtained from above external dataset to kaggle data set. Using the external dataset, It has \\\"reduced\\\" the rmse value. The rmse when external dataset is included is 0.1306. The rmse of the kaggle dataset only is 0.1335. This implies that the crime rate and income impacts the saleprice.**\",\"execution_count\":null},{\"metadata\":{},\"cell_type\":\"markdown\",\"source\":\"## Part 8 - Permutation Test\",\"execution_count\":null},{\"metadata\":{\"trusted\":true},\"cell_type\":\"code\",\"source\":\"from sklearn.preprocessing import LabelEncoder\\nfrom sklearn import linear_model\\nfrom sklearn.model_selection import permutation_test_score\\ntrain3 = pd.read_csv('../input/house-prices-advanced-regression-techniques/train.csv')\\nx = train3\\ny = np.log(train3['SalePrice'])\\n\\ncol_miss = [col for col in x.columns if x[col].isnull().any()]\\nfor col in col_miss:\\n    if(x[col].dtype == np.dtype('object')):\\n         x[col]=x[col].fillna(x[col].value_counts().index[0])\\n    else:\\n        x[col] = x[col].fillna(x[col].mean()) \\nle = LabelEncoder()\\nfor col in x.select_dtypes(include=['object']):\\n    x[col] = le.fit_transform(x[col])\\nlinear_model = linear_model.LinearRegression()\\nx = x.drop(['Id','SalePrice'],axis=1)\\nvariables = ['OverallQual', 'GarageCars', 'GrLivArea','GarageArea','BsmtFullBath','1stFlrSF','FullBath',\\n                'YearBuilt','FireplaceQu','HeatingQC']\\nfor i in range(len(variables)):\\n    a = np.array(x[variables[i]])\\n    a=np.reshape(a,(1460,1))\\n    model = linear_model.fit(a,y)\\n    p_value = permutation_test_score(model,a,y,n_permutations=100)\\n    print(p_value)\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{},\"cell_type\":\"markdown\",\"source\":\"Describe the results\\n* \\n**The p-value, which approximates the probability that the score would be obtained by chance. This is calculated as:\\n    (C + 1) / (n_permutations + 1)\\n   Where C is the number of permutations whose score >= the true score.****\\n* \\n**p-value for each iteration is 0.0099(<0.05), making our hypothesis valid in each case.For C=0, the p-value is therefore 1/101 = 0.0099 as obtained****\",\"execution_count\":null},{\"metadata\":{},\"cell_type\":\"markdown\",\"source\":\"## Part 9 - Final Result\",\"execution_count\":null},{\"metadata\":{\"trusted\":true},\"cell_type\":\"code\",\"source\":\"#Baseline Model\\nfrom sklearn.preprocessing import LabelEncoder\\nfrom sklearn import metrics\\nfrom scipy import stats \\nfrom sklearn.linear_model import Ridge\\nfrom sklearn.model_selection import cross_val_score\\nimport base64\\nimport numpy as np\\nfrom sklearn.preprocessing import LabelEncoder\\nfrom sklearn import linear_model\\nfrom sklearn import metrics\\n\\ntrain = pd.read_csv('../input/house-prices-advanced-regression-techniques/train.csv')\\ntest = pd.read_csv('../input/house-prices-advanced-regression-techniques/test.csv')\\n\\nx = train\\ntest_data = test\\n\\ncol_miss = [col for col in x.columns if x[col].isnull().any()]\\nfor col in col_miss:\\n    if(x[col].dtype == np.dtype('object')):\\n         x[col]=x[col].fillna(x[col].value_counts().index[0])\\n    else:\\n        x[col] = x[col].fillna(x[col].mean()) \\ntest_col_miss = [col for col in test_data.columns if test_data[col].isnull().any()]\\nfor col in test_col_miss:\\n    if(test_data[col].dtype == np.dtype('object')):\\n        test_data[col] = test_data[col].fillna(test_data[col].value_counts().index[0])      \\n    else:\\n        test_data[col] = test_data[col].fillna(test_data[col].median())\\nle = LabelEncoder()\\nfor col in test_data.select_dtypes(include=['object']):\\n    test_data[col] = le.fit_transform(test_data[col]) \\nfor col in x.select_dtypes(include=['object']):\\n    x[col] = le.fit_transform(x[col])\\ny = x[\\\"SalePrice\\\"]\\ny = np.log(y+1)\\nx = x.drop(['Id','SalePrice'],axis=1)\\nlinear_model = linear_model.LinearRegression()\\nmodel = linear_model.fit(x,y)\\ny_pred = model.predict(x)\\nRMSE = np.sqrt(metrics.mean_squared_error(y, y_pred))\\nprint(RMSE)\\nx_test = test_data.drop('Id',axis=1)\\nx_test.shape\\ny_pred_test = model.predict(x_test)\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"trusted\":true},\"cell_type\":\"code\",\"source\":\"#Advanced Model\\nfrom sklearn.preprocessing import LabelEncoder\\nfrom sklearn import metrics\\nfrom scipy import stats \\nfrom sklearn.linear_model import Lasso\\nfrom sklearn.model_selection import cross_val_score\\nimport base64\\n\\ntrain = pd.read_csv('../input/house-prices-advanced-regression-techniques/train.csv')\\ntest = pd.read_csv('../input/house-prices-advanced-regression-techniques/test.csv')\\nx = train\\ntest_data = test\\n\\ncol_miss = [col for col in x.columns if x[col].isnull().any()]\\nfor col in col_miss:\\n    if(x[col].dtype == np.dtype('object')):\\n         x[col]=x[col].fillna(x[col].value_counts().index[0])\\n    else:\\n        x[col] = x[col].fillna(x[col].mean()) \\ntest_col_miss = [col for col in test_data.columns if test_data[col].isnull().any()]\\nfor col in test_col_miss:\\n    if(test_data[col].dtype == np.dtype('object')):\\n        test_data[col] = test_data[col].fillna(test_data[col].value_counts().index[0])      \\n    else:\\n        test_data[col] = test_data[col].fillna(test_data[col].median())\\nle = LabelEncoder()\\nfor col in test_data.select_dtypes(include=['object']):\\n    test_data[col] = le.fit_transform(test_data[col]) \\nfor col in x.select_dtypes(include=['object']):\\n    x[col] = le.fit_transform(x[col])\\n    \\ny = x[\\\"SalePrice\\\"]\\nx = x.drop(['Id','SalePrice'],axis=1)\\nx.shape\\ny = np.log(y+1)\\nLSR = Lasso(alpha=0.0005)\\nLSR.fit(x, y)\\ny_pred = LSR.predict(x)\\nRMSE = np.sqrt(metrics.mean_squared_error(y, y_pred))\\nprint(RMSE)\\nresidual = y - y_pred\\nz_score = np.abs(stats.zscore(residual))\\noutliers=np.where(abs(z_score) > abs(z_score).std() * 3)[0]\\nx = x.drop(outliers)\\ny = y.drop(outliers)\\nLSR.fit(x, y)\\ny_pred = LSR.predict(x)\\nRMSE = np.sqrt(metrics.mean_squared_error(y, y_pred))\\nprint(RMSE)\\nx_test = test_data.drop('Id',axis=1)\\nx_test.shape\\ny_pred_test = LSR.predict(x_test)\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{},\"cell_type\":\"markdown\",\"source\":\"1. **Applied the  Label encoding,linear regression, laso regularization, removed the outliers on the dataset.**\\n1. **On applying the baseline model, An rmse value of 0.1335772078337922 is obtained.**\\n1. **Laso regularization has yielded an rmse of 0.13450417167028753.**\\n1. **After the above steps, the removal of outliers has reduced the value further to 0.09333524067618566**\",\"execution_count\":null},{\"metadata\":{},\"cell_type\":\"markdown\",\"source\":\"Report the rank, score, number of entries, for your highest rank. Include a snapshot of your best score on the leaderboard as confirmation. Be sure to provide a link to your Kaggle profile. Make sure to include a screenshot of your ranking. Make sure your profile includes your face and affiliation with SBU.\",\"execution_count\":null},{\"metadata\":{},\"cell_type\":\"markdown\",\"source\":\"Kaggle Link: https://www.kaggle.com/harshinik97\",\"execution_count\":null},{\"metadata\":{},\"cell_type\":\"markdown\",\"source\":\"Score: 0.11971\",\"execution_count\":null},{\"metadata\":{},\"cell_type\":\"markdown\",\"source\":\"Number of entries: 2\",\"execution_count\":null},{\"metadata\":{},\"cell_type\":\"markdown\",\"source\":\"INCLUDE IMAGE OF YOUR KAGGLE RANKING\",\"execution_count\":null}],\"metadata\":{\"kernelspec\":{\"language\":\"python\",\"display_name\":\"Python 3\",\"name\":\"python3\"},\"language_info\":{\"pygments_lexer\":\"ipython3\",\"nbconvert_exporter\":\"python\",\"version\":\"3.6.4\",\"file_extension\":\".py\",\"codemirror_mode\":{\"name\":\"ipython\",\"version\":3},\"name\":\"python\",\"mimetype\":\"text/x-python\"}},\"nbformat\":4,\"nbformat_minor\":4}\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/house-prices-advanced-regression-techniques/train.csv')\ntest = pd.read_csv('../input/house-prices-advanced-regression-techniques/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train1 = train #make a copy\ntest1 = test\ntrain1 = train1.drop(\"Id\", axis = 1)\ntest1 = test1.drop(\"Id\", axis = 1)\nneighbour_feat = train['Neighborhood']\nsalePrice_arr = train['SalePrice']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fill_none = [ \"MiscFeature\",\"MasVnrType\"] #fill none from the given data\nfor col in fill_none:\n    train1[col] = train1[col].fillna(\"None\")\n    test1[col] = test1[col].fillna(\"None\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fill_na = [\"GarageType\", \"GarageFinish\", \"GarageQual\",\"FireplaceQu\",\"PoolQC\",\"Alley\", \n           \"BsmtQual\", \"BsmtCond\", \"BsmtExposure\", \"BsmtFinType1\",\"BsmtFinType2\",\n                    \"GarageCond\", \"Fence\"]\nfor col in fill_na:\n    train1[col] = train1[col].fillna(\"NA\")\n    test1[col] = test1[col].fillna(\"NA\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dict_moSold = {1 : \"Jan\", 2 : \"Feb\", 3 : \"Mar\", 4 : \"Apr\", 5 : \"May\", 6 : \"Jun\", 7 : \"Jul\", 8 : \"Aug\", 9 : \"Sep\", 10 : \"Oct\",\n         11 : \"Nov\", 12 : \"Dec\"}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"col_missing = []\ncol_missing_test = []\n\nfor col in train1.columns:\n    if len(np.unique(train1.isnull()))>1:\n        col_missing.append(col)\nfor col_miss in col_missing:\n    if train1[col_miss].dtype=='int64':\n        train1[col_miss] = np.nanmean(train1[col_miss])\n    if train1[col_miss].dtype=='float64':\n        train1[col_miss] = np.nanmean(train1[col_miss])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"        \nfor col in test1.columns:\n    if len(np.unique(test1.isnull()))>1:\n        col_missing_test.append(col)\nfor col_miss in col_missing_test:\n    if test1[col_miss].dtype=='int64':\n        test1[col_miss] = np.nanmean(test1[col_miss])\n    if test1[col_miss].dtype=='float64':\n        test1[col_miss] = np.nanmean(test1[col_miss])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndict_msClass = {20 : \"Cat_20\", 30 : \"Cat_30\", 40 : \"Cat_40\", 45 : \"Cat_45\",50 : \"Cat_50\", 60 : \"Cat_60\", 70 : \"Cat_70\", 75 : \"Cat_75\",\n         80 : \"Cat_80\", 85 : \"Cat_85\", 90 : \"Cat_90\", 120 : \"Cat_120\", 150 : \"Cat_150\", 160 : \"Cat_160\", 180 : \"Cat_180\",\n         190 : \"Cat_190\"}\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train1 = train1.replace({\"MSSubClass\" : dict_msClass,\"MoSold\" : dict_moSold})\ntest1 = test1.replace({\"MSSubClass\" : dict_msClass,\"MoSold\" : dict_moSold})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train1.drop(\"LotFrontage\", axis=1, inplace=True)\ntest1.drop(\"LotFrontage\", axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mode_fill = ['Functional','KitchenQual','MSZoning','SaleType','Utilities','Electrical', 'Exterior1st', 'Exterior2nd']\nfor col in mode_fill:\n    train1[col]=train1[col].fillna(train1[col].mode()[0])\n    test1[col]=test1[col].fillna(test1[col].mode()[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train1[\"FinalBath\"] = train1[\"FullBath\"] + ((1/2) * train1[\"HalfBath\"]) + train1[\"BsmtFullBath\"] + ((1/2)*train1[\"BsmtHalfBath\"])\ntest1[\"FinalBath\"] = test1[\"FullBath\"] + ((1/2) * test1[\"HalfBath\"]) + test1[\"BsmtFullBath\"] + ((1/2)*test1[\"BsmtHalfBath\"])\n\ntrain1[\"FinalSF\"] = train1[\"GrLivArea\"] + train1[\"TotalBsmtSF\"]\ntest1[\"FinalSF\"] = test1[\"GrLivArea\"] + test1[\"TotalBsmtSF\"]\n\ntrain1[\"FinalFlrSF\"] = train1[\"1stFlrSF\"] + train1[\"2ndFlrSF\"]\ntest1[\"FinalFlrSF\"] = test1[\"1stFlrSF\"] + test1[\"2ndFlrSF\"]\n\ntrain1[\"FinalPorchSF\"] = train1[\"3SsnPorch\"] + train1[\"ScreenPorch\"] + train1[\"OpenPorchSF\"] + train1[\"EnclosedPorch\"]\ntest1[\"FinalPorchSF\"] = test1[\"3SsnPorch\"] + test1[\"ScreenPorch\"] + test1[\"OpenPorchSF\"] + test1[\"EnclosedPorch\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_label = np.log(train1['SalePrice'])\ntrain1.drop(['SalePrice'], axis = 1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"categ_train = train1.select_dtypes(include=[np.object])\ncateg_test = train1.select_dtypes(include=[np.object])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"categ_train_test = pd.concat([categ_train,categ_test])\ncateg_onehot = pd.get_dummies(categ_train_test, columns=categ_train_test.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"categ_train_encod = categ_onehot[:categ_train.shape[0]]\ncateg_test_encod = categ_onehot[categ_train.shape[0]:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_final = pd.concat([train1, categ_train_encod],axis=1)\ntest_final = pd.concat([test1, categ_test_encod],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_final = train_final.select_dtypes(exclude=['object'])\ntest_final = test_final.select_dtypes(exclude=['object'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Homework 3 - Ames Housing Dataset","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"For all parts below, answer all parts as shown in the Google document for Homework 3. Be sure to include both code that justifies your answer as well as text to answer the questions. We also ask that code be commented to make it easier to follow.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Part 1 - Pairwise Correlations","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# TODO: show visualization\n#correlation matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ncorrmat = train.corr(method='pearson')\nf, ax = plt.subplots(figsize=(14, 11))\nsns.heatmap(corrmat,vmin=-0.5, vmax=0.8, square=True);\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Pearson Correlation for 16 most interesting variables. The top 8 variables which are having best positive correlation and top 8 variables that have the best negative correlation are chosen and heatmap is plotted below**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"k = 8 #number of variables for heatmap\ncols = corrmat.nlargest(k, 'SalePrice')['SalePrice'].index\nsm_value = corrmat.nsmallest(k, 'SalePrice')['SalePrice'].index\nsm_la_col = list(cols)+list(sm_value)\ncm = np.corrcoef(train[sm_la_col].values.T)\nsns.set(font_scale=1.05)\nf, ax = plt.subplots(figsize=(14, 11))\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 11}, yticklabels=sm_la_col, xticklabels=sm_la_col, cmap=\"YlGnBu\")\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Discuss most positive and negative correlations.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sm_la_col #list of 16 of the most interesting variables","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Out of 16 features, the following were found to be extremes when pearson correlation is applied\nHighest correlation: 0.88 between GarageArea and GarageCars\nLowest correlation: -0.25 between MSSubClass and 1stFlrSF**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Part 2 - Informative Plots","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"What interesting properties does Plot 1 reveal?","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\ntrain = pd.read_csv('../input/house-prices-advanced-regression-techniques/train.csv')\ntest = pd.read_csv('../input/house-prices-advanced-regression-techniques/test.csv')\nsns.set_palette(\"husl\")\nsns.set(style=\"darkgrid\")\nplt.scatter(train['GrLivArea'], train['SalePrice'])\nplt.title(\"Above Ground Living Area Vs SalePrice\")\nplt.xlabel(\"Above Ground Living Area\")\nplt.ylabel(\"Sale Price\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**From the above plot, It is evident that, For values of the GrLivArea that are greater than 4000, linear relation is deviated for the 2 points.Thus they are the outliers.\nThe outliers imply that the 2 points might correspond to unused property or the property that is not related to general housing.\n**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set_palette(\"husl\")\nquality = train['SalePrice'].groupby(train['OverallQual']).mean()\nplt.bar(range(0,10),quality)\nplt.xlabel('Quality of house')\nplt.ylabel(' mean Sale Price')\nplt.title('Quality of house vs mean Sale Price')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**The SalePrice increased with Overall Quality as expected. SalePrice follows a steep increasing trend with Overall Quality**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set_palette(\"cubehelix\")\nsns.catplot(x=\"OverallCond\", y=\"SalePrice\",\n            kind=\"box\", dodge=False, data=train, height=10, palette=sns.color_palette(\"coolwarm\", 8))\nplt.xticks(rotation=45);\nplt.title('OverallCond Vs SalePrice')\n\nplt.figure(figsize = (10, 6))\nsns.countplot(x ='OverallCond', data = train, palette=sns.color_palette(\"coolwarm\", 8))\nplt.xticks(rotation=45)\nplt.title('OverallCond')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Overall Condition at 5 seems to be in majority, probably the reason for which there's high SalePrice. The SalePrice seems to increase with Overall Condition on a whole**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"neighborhood = np.unique(train[\"Neighborhood\"])\navg_list_neighbors=[]\nfor neigh in neighborhood:\n    avg_list_neighbors.append(np.mean(train[train[\"Neighborhood\"]\n                                       ==neigh][\"SalePrice\"]))\nplt.plot(neighborhood, avg_list_neighbors)\nplt.xticks(rotation=90);\nplt.xlabel('Neighborhood')\nplt.ylabel('Average')\nplt.title('NeighborHood Vs Average value of Saleprice')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**From the above plot we can observe that NoRidge,DridgHt, StoneBr have maximum average value of saleprice compared to all other neighborhoods. **","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\ntrain.plot(x='MSSubClass', y='SalePrice', kind = 'scatter',xticks=range(0,190,10),figsize = (10, 6))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"** From the given data,\n 20 indicates One storied 1946 and newer all styles\n 60 indicates two storied 1946 and newer\n 120 indicate one storied Planned Unit Development - 1946 and newer.\n MS_SubClass-60 is having higher saleprice compared to other sub-classes followed by MS_SubClass-20 and  MS_SubClass-120.**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Part 3 - Handcrafted Scoring Function","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nscore_params = ['GrLivArea', 'YearBuilt', 'GarageArea', 'OverallCond']\ndef score(data,params):\n    return ((data[params[0]]/(np.mean(data[params[0]])))**2 + data[params[1]]/(np.mean(data[params[1]])) + \\\n            data[params[2]]/(np.mean(data[params[2]])) + (data[params[3]]/(np.mean(data[params[3]]))))\ntrain['desirability'] = score(train,score_params)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainsortdec = train.sort_values(by=['desirability'], ascending=False)\ntrainsortdec.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"What is the ten most desirable houses?\n* ** \"high ground(grade) living area, the year built, garage area, overall condition\" are considered highly desirable attributes. the above 10 are the most desirable based on these attributes.Their saleprice is in good terms with desirability**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"trainsort = train.sort_values(by=['desirability'])\ntrainsort.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"What is the ten least desirable houses?\n* ** \"low ground(grade) living area, the year built, garage area, overall condition\" are considered least desirable attributes.the above table are the ten least desirable based on these factors.Their saleprice is in good terms with desirability and Not very well for low desirable data.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train['SalePrice'].corr(train['desirability'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Describe your scoring function and how well you think it worked.\n* **\n* **As per the above correlation between the desirability with sale price is good, although it needn't be true always. We defined a house desirable having large spaces. With 0.737(rounded) correlation, the scoring function appears to have done fine.****","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Part 4 - Pairwise Distance Function","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"len(train_final.values[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nfrom sklearn import preprocessing\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\n\ndef pairwise_euclidean(a,b):\n    if len(a) == len(b):\n        return np.sqrt(sum([(x-y)**2 for x,y in zip(a,b)]))\n    else:\n        return False\n\nx = train_final.values\nmin_max_scaler = preprocessing.MinMaxScaler()\nx_scaled = min_max_scaler.fit_transform(x)\ntrain = pd.DataFrame(x_scaled,columns=train_final.columns)\npca = PCA(n_components=50)\npcaComponents = pca.fit_transform(train)\npcaDf = pd.DataFrame(data = pcaComponents)\nX_embed = TSNE(n_components=2).fit_transform(pcaDf)\n\npairwiseds = []\nfor i in range(len(X_embed)):\n    ds = []\n    for j in range(len(X_embed)):\n        ds.append(pairwise_euclidean(X_embed[i], X_embed[j]))\n    pairwiseds.append(ds)\n\ntrue_count = 0\nfalse_count = 0\nfor i in range(len(pairwiseds)):\n    original_arr = pairwiseds[i]\n    sorted_arr = np.sort(pairwiseds[i])\n    found_elem = np.where(original_arr == sorted_arr[1])[0]\n    if neighbour_feat[found_elem[0]]==neighbour_feat[i]:\n        true_count=true_count+1\n    else:\n        false_count=false_count+1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(true_count/(true_count+false_count))*100","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"How well does the distance function work? When does it do well/badly?\n* \n**The function got approximately 52% match with neighbourhood which tells that it has done well.\nMin-Max Scalar Normalization technique is used. Dimensionality reduction is done using PCA and reduced 300+ dimensions to 50.Used t-sne to reduce dimensions from 50 to 2.Obtained pair wise distance vector using handcrafted function.Validate whether its nearest distance point contains same neighborhood. If neighborhood is same, increment true_count to true.Approximately 52% of the neighbourhood matches the nearest. **","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Part 5 - Clustering","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.DataFrame(X_embed, columns=['xcoord','ycoord'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plt.scatter(df['xcoord'], df['ycoord'],c='DarkBlue')\nplt.figure(figsize=[12,8])\nsns.scatterplot(x=\"xcoord\", y=\"ycoord\",data=df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.cluster import KMeans\nkmeans = KMeans(n_clusters=9, random_state=0).fit(X_embed)\nlabels = kmeans.labels_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['labels'] = labels\nplt.figure(figsize=[12,8])\nsns.scatterplot(x=\"xcoord\", y=\"ycoord\",hue=\"labels\",legend=\"full\",palette=sns.color_palette(\"hls\", 9),data=df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport operator\n#df_main = pd.DataFrame()\nfor i in range(0,9):\n    neighbor = neighbour_feat[labels==i]\n    unique, counts = np.unique(neighbor, return_counts=True)\n    d = dict(zip(unique, counts))\n    sorted_d = sorted(d.items(), key=operator.itemgetter(1),reverse=True)\n    df_ = pd.DataFrame({'clustercentre': i,'Neighborhood': sorted_d[0][0],'count':sorted_d[0][1],\n                       'percent':(sorted_d[0][1]/sum(d.values()))*100}, index = [i])\n    print(df_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.cluster import AgglomerativeClustering\nclustering = AgglomerativeClustering(affinity='precomputed', compute_full_tree='auto',\n                       connectivity=None, distance_threshold=None,\n                       linkage='single', memory=None, n_clusters=9,\n                       pooling_func='deprecated').fit(pairwiseds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['pre_labels'] = clustering.labels_\npre_labels = clustering.labels_\nplt.figure(figsize=[12,8])\nsns.scatterplot(x=\"xcoord\", y=\"ycoord\",hue=\"pre_labels\",legend=\"full\",palette=sns.color_palette(\"hls\", 9),data=df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport operator\n#df_main = pd.DataFrame()\nfor i in range(0,9):\n    neighbor = neighbour_feat[pre_labels==i]\n    unique, counts = np.unique(neighbor, return_counts=True)\n    d = dict(zip(unique, counts))\n    sorted_d = sorted(d.items(), key=operator.itemgetter(1),reverse=True)\n    df_ = pd.DataFrame({'clustercentre': i,'Neighborhood': sorted_d[0][0],'count':sorted_d[0][1],\n                       'percent':(sorted_d[0][1]/sum(d.values()))*100}, index = [i])\n    print(df_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"How well do the clusters reflect neighborhood boundaries? Write a discussion on what your clusters capture and how well they work.\n\n**We applied two algorithms here, kmeans and Agglomerative. Kmeans takes input as X_embed(reduced dimensionality), which is the output of tsne. Kmeans makes sure every cluster is uniformly distributed. However, we have repeating maximum neighborhoods. In this case for example, we have 'NAmes', coming in 3 clusters'. Agglomerative takes distance matrix as input and cluster labels, from which we determine the cluster it belongs too. As we can see from graphs above, agglomerative does non-uniform clustering with 1 cluster taking all 'NAmes' data. However, the clusters have a clear separation of the neighborhoods .**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Part 6 - Linear Regression","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import linear_model\nfrom sklearn import metrics\nfrom sklearn.preprocessing import LabelEncoder\n\ntrain2 = pd.read_csv('../input/house-prices-advanced-regression-techniques/train.csv')\nlinear_model = linear_model.LinearRegression()\n#x = x.drop(['Id','SalePrice'],axis=1)\nx = train2\nx = x.drop(['Id','SalePrice'],axis=1)\ny = salePrice_arr\n\ncol_miss = [col for col in x.columns if x[col].isnull().any()]\nfor col in col_miss:\n    if(x[col].dtype == np.dtype('object')):\n         x[col]=x[col].fillna(x[col].value_counts().index[0])\n    else:\n        x[col] = x[col].fillna(x[col].mean()) \nle = LabelEncoder()\nfor col in x.select_dtypes(include=['object']):\n    x[col] = le.fit_transform(x[col])\n    \nmodel = linear_model.fit(x,y)\ny_fit = model.predict(x)\nRMSE = np.sqrt(metrics.mean_squared_error(y, y_fit))\nRMSE","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_ = np.log(y)\nmodel = linear_model.fit(x,y_)\ny_fit = model.predict(x)\nRMSE = np.sqrt(metrics.mean_squared_error(y_, y_fit))\nRMSE","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import operator\ncoeffs = model.coef_\nmax_index, max_value = max(enumerate(coeffs), key=operator.itemgetter(1))\nmin_index, min_value = min(enumerate(coeffs), key=operator.itemgetter(1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x.columns[max_index]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_value","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x.columns[min_index]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"min_value","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"How well/badly does it work? Which are the most important variables?\n* \n**The initial rmse value is 30000 which isn't a good value. To yeild better results the fitting against the log of the value and label encoding is applied thus rmse got reduced. The new rmse value is approximately equal to 0.1335. poolqc has the highest absolute value of coefficient and Street has the highest positive value.High weights contribute a lot to the model and hence are most important**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Part 7 - External Dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\ntrain3 = pd.read_csv('../input/house-prices-advanced-regression-techniques/train.csv')\n\nx = train3\nnational_burglary_rate = 430.4\nnational_rape_rate = 41.7\nnational_murder_rate = 5.3\n#In case of missing data, we substituted with Ames city average\n#https://www.addressreport.com/report/neighborhood/ames-ia/bloomington-heights-ames-ia/\n\n#first five are household_income,owners,property_tax and remaining are understood\n\ndict_ = {}\n\names_avg = [46358,0.44,2479,\n                (1-0.30)*national_burglary_rate,(1-0.0)*national_rape_rate,\n                (1-0.81)*national_murder_rate]\ndict_['Blmngtn'] = [95256,0.84,3742,\n                      (1-0.05)*national_burglary_rate,(1-0.54)*national_rape_rate,\n                      (1-0.79)*national_murder_rate]\ndict_['Blueste'] = ames_avg\ndict_['BrDale'] = [45558,0.40,1785,\n                      (1-0.14)*national_burglary_rate,(1-0.26)*national_rape_rate,\n                      (1-0.89)*national_murder_rate]\ndict_['BrkSide'] = ames_avg\ndict_['ClearCr'] = ames_avg\ndict_['CollgCr'] = [66875,0.55,2616,\n                    (1-0.26)*national_burglary_rate,(1+0.11)*national_rape_rate,\n                (1-0.92)*national_murder_rate]\ndict_['Crawfor'] = ames_avg\ndict_['Edwards'] = ames_avg\ndict_['Gilbert'] = ames_avg\ndict_['IDOTRR'] = ames_avg\ndict_['MeadowV'] = [53962,0.51,1521,\n            (1-0.52)*national_burglary_rate,(1+0.22)*national_rape_rate,\n            (1-0.78)*national_murder_rate]\ndict_['Mitchel'] = ames_avg\ndict_['NAmes'] = ames_avg\ndict_['NoRidge'] = ames_avg\ndict_['NPkVill'] = ames_avg\ndict_['NridgHt'] = [95256,0.84,5478,\n            (1-0.05)*national_burglary_rate,(1-0.54)*national_rape_rate,\n            (1-0.79)*national_murder_rate]\ndict_['NWAmes'] = ames_avg\ndict_['OldTown'] = ames_avg\ndict_['SWISU'] = ames_avg\ndict_['Sawyer'] = ames_avg\ndict_['SawyerW'] = ames_avg\ndict_['Somerst'] = [84600,0.62,2314,\n            (1+0.09)*national_burglary_rate,(1-0.39)*national_rape_rate,\n            (1-0.78)*national_murder_rate]\ndict_['StoneBr'] = [95256,0.84,4369,\n            (1-0.05)*national_burglary_rate,(1-0.54)*national_rape_rate,\n            (1-0.79)*national_murder_rate]\ndict_['Timber'] = ames_avg\ndict_['Veenker'] = ames_avg\nx['household_income'] = 0\nx['owners'] = 0\nx['property_tax'] = 0\nx['burglary_rate'] = 0\nx['rape_rate'] = 0\nx['murder_rate'] = 0\nkeyset = ['household_income','owners','property_tax','burglary_rate','rape_rate','murder_rate']\nhousehold_arr = []\nowners_arr = []\nproperty_tax_arr=[]\nburglary_rate_arr=[]\nrape_rate_arr=[]\nmurder_rate_arr=[]\nfor i in range(len(x)):\n    neighborhood = x.iloc[i]['Neighborhood']\n    household_arr.append(dict_[neighborhood][0])\n    owners_arr.append(dict_[neighborhood][1])\n    property_tax_arr.append(dict_[neighborhood][2])\n    burglary_rate_arr.append(dict_[neighborhood][3])\n    rape_rate_arr.append(dict_[neighborhood][4])\n    murder_rate_arr.append(dict_[neighborhood][5])\nx['household_income']=household_arr\nx['owners']=owners_arr\nx['property_tax']=property_tax_arr\nx['burglary_rate']=burglary_rate_arr\nx['rape_rate']=rape_rate_arr\nx['murder_rate']=murder_rate_arr","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Baseline Model\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn import metrics\nfrom scipy import stats \nfrom sklearn.linear_model import Ridge\nfrom sklearn.model_selection import cross_val_score\nimport base64\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn import linear_model\nfrom sklearn import metrics\n\ncol_miss = [col for col in x.columns if x[col].isnull().any()]\nfor col in col_miss:\n    if(x[col].dtype == np.dtype('object')):\n         x[col]=x[col].fillna(x[col].value_counts().index[0])\n    else:\n        x[col] = x[col].fillna(x[col].mean()) \nle = LabelEncoder()\nfor col in x.select_dtypes(include=['object']):\n    x[col] = le.fit_transform(x[col])\n    \ny = x[\"SalePrice\"]\ny = np.log(y+1)\nx = x.drop(['Id','SalePrice'],axis=1)\nlm = linear_model.LinearRegression()\nmodel = lm.fit(x,y)\ny_pred = model.predict(x)\nRMSE = np.sqrt(metrics.mean_squared_error(y, y_pred))\nprint(RMSE)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Describe the dataset and whether this data helps with prediction.\n* \n** The above dataset is procured from #https://en.wikipedia.org/wiki/Crime_in_the_United_States.  the data  is processed using https://www.addressreport.com and extracted values for attributes like household_income,owners,property_tax, burglary_rate,rape_rate, murder_rate for available neighborhoods of the Ames. The data that is missing is replaced with the city average. The Linear regression model is then applied after appending the attributes obtained from above external dataset to kaggle data set. Using the external dataset, It has \"reduced\" the rmse value. The rmse when external dataset is included is 0.1306. The rmse of the kaggle dataset only is 0.1335. This implies that the crime rate and income impacts the saleprice.**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Part 8 - Permutation Test","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nfrom sklearn import linear_model\nfrom sklearn.model_selection import permutation_test_score\ntrain3 = pd.read_csv('../input/house-prices-advanced-regression-techniques/train.csv')\nx = train3\ny = np.log(train3['SalePrice'])\n\ncol_miss = [col for col in x.columns if x[col].isnull().any()]\nfor col in col_miss:\n    if(x[col].dtype == np.dtype('object')):\n         x[col]=x[col].fillna(x[col].value_counts().index[0])\n    else:\n        x[col] = x[col].fillna(x[col].mean()) \nle = LabelEncoder()\nfor col in x.select_dtypes(include=['object']):\n    x[col] = le.fit_transform(x[col])\nlinear_model = linear_model.LinearRegression()\nx = x.drop(['Id','SalePrice'],axis=1)\nvariables = ['OverallQual', 'GarageCars', 'GrLivArea','GarageArea','BsmtFullBath','1stFlrSF','FullBath',\n                'YearBuilt','FireplaceQu','HeatingQC']\nfor i in range(len(variables)):\n    a = np.array(x[variables[i]])\n    a=np.reshape(a,(1460,1))\n    model = linear_model.fit(a,y)\n    p_value = permutation_test_score(model,a,y,n_permutations=100)\n    print(p_value)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Describe the results\n* \n**The p-value, which approximates the probability that the score would be obtained by chance. This is calculated as:\n    (C + 1) / (n_permutations + 1)\n   Where C is the number of permutations whose score >= the true score.****\n* \n**p-value for each iteration is 0.0099(<0.05), making our hypothesis valid in each case.For C=0, the p-value is therefore 1/101 = 0.0099 as obtained****","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Part 9 - Final Result","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Baseline Model\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn import metrics\nfrom scipy import stats \nfrom sklearn.linear_model import Ridge\nfrom sklearn.model_selection import cross_val_score\nimport base64\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn import linear_model\nfrom sklearn import metrics\n\ntrain = pd.read_csv('../input/house-prices-advanced-regression-techniques/train.csv')\ntest = pd.read_csv('../input/house-prices-advanced-regression-techniques/test.csv')\n\nx = train\ntest_data = test\n\ncol_miss = [col for col in x.columns if x[col].isnull().any()]\nfor col in col_miss:\n    if(x[col].dtype == np.dtype('object')):\n         x[col]=x[col].fillna(x[col].value_counts().index[0])\n    else:\n        x[col] = x[col].fillna(x[col].mean()) \ntest_col_miss = [col for col in test_data.columns if test_data[col].isnull().any()]\nfor col in test_col_miss:\n    if(test_data[col].dtype == np.dtype('object')):\n        test_data[col] = test_data[col].fillna(test_data[col].value_counts().index[0])      \n    else:\n        test_data[col] = test_data[col].fillna(test_data[col].median())\nle = LabelEncoder()\nfor col in test_data.select_dtypes(include=['object']):\n    test_data[col] = le.fit_transform(test_data[col]) \nfor col in x.select_dtypes(include=['object']):\n    x[col] = le.fit_transform(x[col])\ny = x[\"SalePrice\"]\ny = np.log(y+1)\nx = x.drop(['Id','SalePrice'],axis=1)\nlinear_model = linear_model.LinearRegression()\nmodel = linear_model.fit(x,y)\ny_pred = model.predict(x)\nRMSE = np.sqrt(metrics.mean_squared_error(y, y_pred))\nprint(RMSE)\nx_test = test_data.drop('Id',axis=1)\nx_test.shape\ny_pred_test = model.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Advanced Model\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn import metrics\nfrom scipy import stats \nfrom sklearn.linear_model import Lasso\nfrom sklearn.model_selection import cross_val_score\nimport base64\n\ntrain = pd.read_csv('../input/house-prices-advanced-regression-techniques/train.csv')\ntest = pd.read_csv('../input/house-prices-advanced-regression-techniques/test.csv')\nx = train\ntest_data = test\n\ncol_miss = [col for col in x.columns if x[col].isnull().any()]\nfor col in col_miss:\n    if(x[col].dtype == np.dtype('object')):\n         x[col]=x[col].fillna(x[col].value_counts().index[0])\n    else:\n        x[col] = x[col].fillna(x[col].mean()) \ntest_col_miss = [col for col in test_data.columns if test_data[col].isnull().any()]\nfor col in test_col_miss:\n    if(test_data[col].dtype == np.dtype('object')):\n        test_data[col] = test_data[col].fillna(test_data[col].value_counts().index[0])      \n    else:\n        test_data[col] = test_data[col].fillna(test_data[col].median())\nle = LabelEncoder()\nfor col in test_data.select_dtypes(include=['object']):\n    test_data[col] = le.fit_transform(test_data[col]) \nfor col in x.select_dtypes(include=['object']):\n    x[col] = le.fit_transform(x[col])\n    \ny = x[\"SalePrice\"]\nx = x.drop(['Id','SalePrice'],axis=1)\nx.shape\ny = np.log(y+1)\nLSR = Lasso(alpha=0.0005)\nLSR.fit(x, y)\ny_pred = LSR.predict(x)\nRMSE = np.sqrt(metrics.mean_squared_error(y, y_pred))\nprint(RMSE)\nresidual = y - y_pred\nz_score = np.abs(stats.zscore(residual))\noutliers=np.where(abs(z_score) > abs(z_score).std() * 3)[0]\nx = x.drop(outliers)\ny = y.drop(outliers)\nLSR.fit(x, y)\ny_pred = LSR.predict(x)\nRMSE = np.sqrt(metrics.mean_squared_error(y, y_pred))\nprint(RMSE)\nx_test = test_data.drop('Id',axis=1)\nx_test.shape\ny_pred_test = LSR.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. **Applied the  Label encoding,linear regression, laso regularization, removed the outliers on the dataset.**\n1. **On applying the baseline model, An rmse value of 0.1335772078337922 is obtained.**\n1. **Laso regularization has yielded an rmse of 0.13450417167028753.**\n1. **After the above steps, the removal of outliers has reduced the value further to 0.09333524067618566**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Report the rank, score, number of entries, for your highest rank. Include a snapshot of your best score on the leaderboard as confirmation. Be sure to provide a link to your Kaggle profile. Make sure to include a screenshot of your ranking. Make sure your profile includes your face and affiliation with SBU.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Kaggle Link: https://www.kaggle.com/harshinik97","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Score: 0.11971","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Number of entries: 2","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"INCLUDE IMAGE OF YOUR KAGGLE RANKING","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}